run_name: "sample_sac"
seed: 0 # Seed of the experiment
torch_deterministic: True # If toggled, `torch.backends.cudnn.deterministic=False`
cuda: False # If toggled, cuda will be enabled by default
method: method_name # Method used (only meant for logging purposes).
reward: linear # Reward used (only meant for logging purposes).
train:
  name: [Eplus-5Zone-cool-continuous-v1] # Environment to be trained on
  model: # If using transfer learning
    path: pretrained_path # Path of pretrained model
    env_trained: Eplus-5Zone-hot-continuous-v1 # If input and output layers have to be reset, then provide orignal env name
wandb:
  project: project_name
  entity: entity_name
  api_key: api_key
agent:
  algorithm: "sac" # To identify which algorithm
  checkpoint_freq: 100_000 # After how many total_timesteps to save
  layers: [64, 128, 64] # Intermediate layers
  hyperparams:
    total_timesteps: 500_000 # Total timesteps of the experiment
    buffer_size: "1e6" # The replay memory buffer size
    gamma: 0.99 # The discount factor gamma
    tau: 0.005 # Target smoothing coefficient
    batch_size: 64 # The batch size of sample from the replay memory
    learning_starts: "0" # Timestep to start learning
    policy_lr: "3e-4" # The learning rate of the policy network optimizer
    q_lr: "1e-3" # The learning rate of the Q network network optimizer
    policy_frequency: 2 # The frequency of training policy (delayed)
    target_network_frequency: 2 # The frequency of updates for the target nerworks
    alpha: 0.2 # Entropy regularization coefficient
    autotune: True # Automatic tuning of the entropy coefficient
    exploration_noise: 0.1 # The scale of exploration noise
    noise_clip: 0.5 # Noise clip parameter of the Target Policy Smoothing Regularization